{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097d9fcb-16d5-4744-87e6-7653971788ee",
   "metadata": {},
   "source": [
    "# ğŸŒ Part 1: Environment Setup and Library Imports\n",
    "\n",
    "**Learning Outcome (LO1):**  \n",
    "You will learn to set up a Python environment, import essential libraries, and understand what each does.  \n",
    "\n",
    "> ğŸ’¡ *Tip:* We are using **UBC Syzygy** â€” a cloud Jupyter Notebook environment. No installation is needed, but we must import libraries (aka collections of pre-written code that provide reusable functionalities to assist in developing applications and performing some common operations in your own programs), so that we don't have to write all the code from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad60a-c1e3-494c-bf61-30a66d40f562",
   "metadata": {},
   "source": [
    "# ğŸ“¦ Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547e28a4-0d84-469c-83eb-083c77f7f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (2.2.6)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.13/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy nltk scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7daf8c-a58f-41e8-8ab6-a4183cea2cb0",
   "metadata": {},
   "source": [
    "# ğŸ§° Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e936e781-708d-404e-a539-546953452de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jupyter/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully! ğŸ˜„\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import the Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download('punkt_tab') \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Import tools for tokenization and lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import the Counter class for word frequency counting\n",
    "from collections import Counter  \n",
    "\n",
    "# Import scikit-learn for Bag-of-Words (BoW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download necessary NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully! ğŸ˜„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8204b9d2-04be-41c4-bf3b-9c1f62bf0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4810e1-6d99-47e5-a6c5-bcfc90442bf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ğŸ§¹ Part 2: Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b5ff3-539e-4e7f-8f33-542fac3307dc",
   "metadata": {},
   "source": [
    "# ğŸ“ Step 1: Load the sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfd755-a8a0-4c33-8e15-4265dfdc3bcb",
   "metadata": {},
   "source": [
    "### ğŸ“– The Tell-Tale Heart by Edgar Allan Poe\n",
    "#### Source: https://poemuseum.org/the-tell-tale-heart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec242e5-7768-41c2-a8aa-b57a418ab507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text loaded successfully! Length: 398 characters\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "TRUE! â€” nervous â€” very, very dreadfully nervous I had been and am; but why will you say that I am mad?\n",
    "The disease had sharpened my senses â€” not destroyed â€” not dulled them. Above all was the sense of hearing acute.\n",
    "I heard all things in the heaven and in the earth. I heard many things in hell.\n",
    "How, then, am I mad? Hearken! and observe how healthily â€” how calmly I can tell you the whole story.\n",
    "\"\"\"\n",
    "print(\"âœ… Text loaded successfully! Length:\", len(sample_text), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb7586-adff-4d3d-a04e-9b65bae389d9",
   "metadata": {},
   "source": [
    "# ğŸ§© Step 2: Tokenize text into words\n",
    "#### Tokenization is to convert text into individual terms (tokens) - in this case, we are tokenizing them into single words in a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d238aa6c-b2bc-423b-963e-dd8f6aa34e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRUE',\n",
       " '!',\n",
       " 'â€”',\n",
       " 'nervous',\n",
       " 'â€”',\n",
       " 'very',\n",
       " ',',\n",
       " 'very',\n",
       " 'dreadfully',\n",
       " 'nervous',\n",
       " 'I',\n",
       " 'had',\n",
       " 'been',\n",
       " 'and',\n",
       " 'am',\n",
       " ';',\n",
       " 'but',\n",
       " 'why',\n",
       " 'will',\n",
       " 'you',\n",
       " 'say',\n",
       " 'that',\n",
       " 'I',\n",
       " 'am',\n",
       " 'mad',\n",
       " '?',\n",
       " 'The',\n",
       " 'disease',\n",
       " 'had',\n",
       " 'sharpened',\n",
       " 'my',\n",
       " 'senses',\n",
       " 'â€”',\n",
       " 'not',\n",
       " 'destroyed',\n",
       " 'â€”',\n",
       " 'not',\n",
       " 'dulled',\n",
       " 'them',\n",
       " '.',\n",
       " 'Above',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'hearing',\n",
       " 'acute',\n",
       " '.',\n",
       " 'I',\n",
       " 'heard',\n",
       " 'all',\n",
       " 'things',\n",
       " 'in',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " 'I',\n",
       " 'heard',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'hell',\n",
       " '.',\n",
       " 'How',\n",
       " ',',\n",
       " 'then',\n",
       " ',',\n",
       " 'am',\n",
       " 'I',\n",
       " 'mad',\n",
       " '?',\n",
       " 'Hearken',\n",
       " '!',\n",
       " 'and',\n",
       " 'observe',\n",
       " 'how',\n",
       " 'healthily',\n",
       " 'â€”',\n",
       " 'how',\n",
       " 'calmly',\n",
       " 'I',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HINT: What is the text that we want to use word_tokenize() to tokenize called?\n",
    "tokens = word_tokenize(____________)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694b0c7e-4b2c-4cbc-94a5-7b0ea1b84e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TRUE', 'NN'),\n",
       " ('!', '.'),\n",
       " ('â€”', 'RB'),\n",
       " ('nervous', 'JJ'),\n",
       " ('â€”', 'JJ'),\n",
       " ('very', 'RB'),\n",
       " (',', ','),\n",
       " ('very', 'RB'),\n",
       " ('dreadfully', 'RB'),\n",
       " ('nervous', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('had', 'VBD'),\n",
       " ('been', 'VBN'),\n",
       " ('and', 'CC'),\n",
       " ('am', 'VBP'),\n",
       " (';', ':'),\n",
       " ('but', 'CC'),\n",
       " ('why', 'WRB'),\n",
       " ('will', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('say', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('mad', 'JJ'),\n",
       " ('?', '.'),\n",
       " ('The', 'DT'),\n",
       " ('disease', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('sharpened', 'VBN'),\n",
       " ('my', 'PRP$'),\n",
       " ('senses', 'NNS'),\n",
       " ('â€”', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('destroyed', 'VBN'),\n",
       " ('â€”', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('dulled', 'VB'),\n",
       " ('them', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Above', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('sense', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('hearing', 'VBG'),\n",
       " ('acute', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('heard', 'VBD'),\n",
       " ('all', 'DT'),\n",
       " ('things', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('heaven', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('earth', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('heard', 'VBD'),\n",
       " ('many', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('hell', 'NN'),\n",
       " ('.', '.'),\n",
       " ('How', 'NNP'),\n",
       " (',', ','),\n",
       " ('then', 'RB'),\n",
       " (',', ','),\n",
       " ('am', 'VBP'),\n",
       " ('I', 'PRP'),\n",
       " ('mad', 'JJ'),\n",
       " ('?', '.'),\n",
       " ('Hearken', 'NNP'),\n",
       " ('!', '.'),\n",
       " ('and', 'CC'),\n",
       " ('observe', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('healthily', 'RB'),\n",
       " ('â€”', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('calmly', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('tell', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('the', 'DT'),\n",
       " ('whole', 'JJ'),\n",
       " ('story', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HINT: Add POS (part-of-speech) tag to the tokens you just created\n",
    "# to indicate the word types (e.g., nouns, verbs, adjectives)\n",
    "tagged = pos_tag(___________)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd905d-cf69-4969-ac98-ef13bae4f3b6",
   "metadata": {},
   "source": [
    "# ğŸ§© Step 3: Lemmatize each token\n",
    "#### Lemmatization is an analytical process to extract lemmas (the most basic forms of words) to address the problem of having different words referring to essentially the same meaning but taking different forms (e.g., computer, computing, compute, computers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8bc405-2edc-40a9-ba0d-9c9b0ca223be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: What action do you want the lemmatizer to take here?\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.____________(word, get_wordnet_pos(tag)) for (word, tag) in tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea9860d-c678-4723-a858-e2b979793579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤Original Token      â†’  ğŸ§  Lemma\n",
      "------------------------------\n",
      "TRUE            â†’ TRUE\n",
      "!               â†’ !\n",
      "â€”               â†’ â€”\n",
      "nervous         â†’ nervous\n",
      "â€”               â†’ â€”\n",
      "very            â†’ very\n",
      ",               â†’ ,\n",
      "very            â†’ very\n",
      "dreadfully      â†’ dreadfully\n",
      "nervous         â†’ nervous\n",
      "I               â†’ I\n",
      "had             â†’ have\n",
      "been            â†’ be\n",
      "and             â†’ and\n",
      "am              â†’ be\n",
      ";               â†’ ;\n",
      "but             â†’ but\n",
      "why             â†’ why\n",
      "will            â†’ will\n",
      "you             â†’ you\n",
      "say             â†’ say\n",
      "that            â†’ that\n",
      "I               â†’ I\n",
      "am              â†’ be\n",
      "mad             â†’ mad\n",
      "?               â†’ ?\n",
      "The             â†’ The\n",
      "disease         â†’ disease\n",
      "had             â†’ have\n",
      "sharpened       â†’ sharpen\n",
      "my              â†’ my\n",
      "senses          â†’ sens\n",
      "â€”               â†’ â€”\n",
      "not             â†’ not\n",
      "destroyed       â†’ destroy\n",
      "â€”               â†’ â€”\n",
      "not             â†’ not\n",
      "dulled          â†’ dull\n",
      "them            â†’ them\n",
      ".               â†’ .\n",
      "Above           â†’ Above\n",
      "all             â†’ all\n",
      "was             â†’ be\n",
      "the             â†’ the\n",
      "sense           â†’ sense\n",
      "of              â†’ of\n",
      "hearing         â†’ hear\n",
      "acute           â†’ acute\n",
      ".               â†’ .\n",
      "I               â†’ I\n",
      "heard           â†’ hear\n",
      "all             â†’ all\n",
      "things          â†’ thing\n",
      "in              â†’ in\n",
      "the             â†’ the\n",
      "heaven          â†’ heaven\n",
      "and             â†’ and\n",
      "in              â†’ in\n",
      "the             â†’ the\n",
      "earth           â†’ earth\n",
      ".               â†’ .\n",
      "I               â†’ I\n",
      "heard           â†’ hear\n",
      "many            â†’ many\n",
      "things          â†’ thing\n",
      "in              â†’ in\n",
      "hell            â†’ hell\n",
      ".               â†’ .\n",
      "How             â†’ How\n",
      ",               â†’ ,\n",
      "then            â†’ then\n",
      ",               â†’ ,\n",
      "am              â†’ be\n",
      "I               â†’ I\n",
      "mad             â†’ mad\n",
      "?               â†’ ?\n",
      "Hearken         â†’ Hearken\n",
      "!               â†’ !\n",
      "and             â†’ and\n",
      "observe         â†’ observe\n",
      "how             â†’ how\n",
      "healthily       â†’ healthily\n",
      "â€”               â†’ â€”\n",
      "how             â†’ how\n",
      "calmly          â†’ calmly\n",
      "I               â†’ I\n",
      "can             â†’ can\n",
      "tell            â†’ tell\n",
      "you             â†’ you\n",
      "the             â†’ the\n",
      "whole           â†’ whole\n",
      "story           â†’ story\n",
      ".               â†’ .\n"
     ]
    }
   ],
   "source": [
    "# Print title/header\n",
    "print(\"ğŸ”¤Original Token      â†’  ğŸ§  Lemma\")\n",
    "print(\"-\" * 30)  # Optional: a separator line\n",
    "\n",
    "#HINT: Zip original tokens and their lemmas together\n",
    "for original, lemma in _______(tokens, lemmas):\n",
    "    print(f\"{original:15} â†’ {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca8deb-6041-48ed-862e-6d55f1fc5bb1",
   "metadata": {},
   "source": [
    "# ğŸ” Can You Spot the Difference?\n",
    "\n",
    "**Learning Outcome (LO2):**  \n",
    "Perform tokenization using lemmatization to clean up text for NLP.\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ§ **Think:** What has changed from one original token to its lemma? How can this possibly facilitate natural language processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347534d-c7af-44e4-a961-90ce88d9a863",
   "metadata": {},
   "source": [
    "# âœ… TEST 1: Check if lemmatization was applied correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13268115-98e5-4470-b409-fbe00b58eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜„ PASSED: Your text was tokenized and lemmatized correctly!\n",
      "ğŸ”¢ Total tokens found: 93\n",
      "ğŸ§  Sample lemmas: ['TRUE', '!', 'â€”', 'nervous', 'â€”', 'very', ',', 'very', 'dreadfully', 'nervous']\n"
     ]
    }
   ],
   "source": [
    "# Basic checks\n",
    "conditions = [\n",
    "    isinstance(tokens, list),                    # tokens should be a list\n",
    "    len(tokens) > 20,                            # enough tokens should be found\n",
    "    isinstance(lemmas, list),                    # lemmas should be a list\n",
    "    all(isinstance(x, str) for x in lemmas),     # all lemmas should be strings\n",
    "]\n",
    "\n",
    "# Display friendly feedback\n",
    "if all(conditions):\n",
    "    print(\"ğŸ˜„ PASSED: Your text was tokenized and lemmatized correctly!\")\n",
    "    print(f\"ğŸ”¢ Total tokens found: {len(tokens)}\")\n",
    "    print(f\"ğŸ§  Sample lemmas: {lemmas[:10]}\")\n",
    "else:\n",
    "    print(\"âŒ TRY AGAIN: Something's off. Check if you filled in the blanks correctly for tokenization or lemmatization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da667b-b2fc-4551-addc-7be224394146",
   "metadata": {},
   "source": [
    "# ğŸ§° Part 3: Bag-of-Words (BoW) Model\n",
    "\n",
    "**Learning Outcomes (LO3 & LO4):**  \n",
    "Understand the Bag-of-Words (BoW) model and use it to convert text into a numerical format that a computer can process (NLP) & Solve problems during the coding process using online tools through collaboration.\n",
    "\n",
    "\n",
    "> **Your task:** Fill in blanks to transform text into a matrix of word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c509830-d424-4cfb-8678-87a7cef83906",
   "metadata": {},
   "source": [
    "# ğŸ§© Step 1: Create a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c004ad3-f12e-49f8-9bf7-e50b69e7acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to turn the text into a numeric bag-of-words vector and to remove the stop-words\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True) \n",
    "\n",
    "# Stop-words are common words like prepositions (in, on, at), conjunctions (and, or, but), articles (a, an, the), \n",
    "# and determiners (this, that, any) that tend to carry little information on what the document could be about\n",
    "# as opposed to nouns, verbs, and adjectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f815c9-1763-489f-b1c5-754a72f7607b",
   "metadata": {},
   "source": [
    "# ğŸ§© Step 2: Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0a70515-c075-4c02-a87d-c5bb6fd67e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: What is the text that you have been focusing on so far called?\n",
    "X = vectorizer.fit_transform([______________])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400566c-2422-4155-aed4-3b80f717da8b",
   "metadata": {},
   "source": [
    "# ğŸ§© Step 3: Get feature (word) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2c2dc8-4210-4ad3-9001-a299e905882d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acute', 'calmly', 'destroyed', 'disease', 'dreadfully', 'dulled',\n",
       "       'earth', 'healthily', 'heard', 'hearing', 'hearken', 'heaven',\n",
       "       'hell', 'mad', 'nervous', 'observe', 'say', 'sense', 'senses',\n",
       "       'sharpened', 'story', 'tell', 'things', 'true'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names_out()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d4f01a-8b74-4ec5-b374-05dcd8bdfc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Vocabulary: ['acute' 'calmly' 'destroyed' 'disease' 'dreadfully' 'dulled' 'earth'\n",
      " 'healthily' 'heard' 'hearing' 'hearken' 'heaven' 'hell' 'mad' 'nervous'\n",
      " 'observe' 'say' 'sense' 'senses' 'sharpened' 'story' 'tell' 'things'\n",
      " 'true']\n",
      "ğŸ§® BoW Matrix:\n",
      " [[1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"ğŸ”¤ Vocabulary:\", words)\n",
    "print(\"ğŸ§® BoW Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69485f1-8bb2-4e16-a335-2c5bb0b74cf8",
   "metadata": {},
   "source": [
    "# ğŸ§ª TEST 2 â€” Check if Bag-of-Words model is correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff01fa4-def0-4cab-a0e2-17017aa78e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜„ PASSED: Bag-of-Words created successfully!\n",
      "ğŸ“Š Vocabulary size: 24\n",
      "ğŸ§® BoW matrix shape: (1, 24)\n",
      "ğŸ”¤ Sample words: ['acute' 'calmly' 'destroyed' 'disease' 'dreadfully' 'dulled' 'earth'\n",
      " 'healthily' 'heard' 'hearing']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_words = len(words)\n",
    "    matrix_shape = X.shape\n",
    "\n",
    "    if num_words > 10 and matrix_shape[1] == num_words:\n",
    "        print(\"ğŸ˜„ PASSED: Bag-of-Words created successfully!\")\n",
    "        print(f\"ğŸ“Š Vocabulary size: {num_words}\")\n",
    "        print(f\"ğŸ§® BoW matrix shape: {matrix_shape}\")\n",
    "        print(f\"ğŸ”¤ Sample words: {words[:10]}\")\n",
    "    else:\n",
    "        print(\"âŒ TRY AGAIN: BoW model doesn't look right. Make sure to use vectorizer.fit_transform([sample_text]) and get_feature_names_out().\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ ERROR: Something went wrong.\")\n",
    "    print(\"ğŸ’¡ HINT: Check if 'vectorizer', 'X', and 'words' are defined correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69636f25-5631-4893-87d5-b30c84dd53aa",
   "metadata": {},
   "source": [
    "# ğŸ§¾ Step 4: Display word counts so you can SEE how BoW works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cc0b4d6-2ae8-47ce-8b90-eb9793d8dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Word Count (Bag-of-Words Representation):\n",
      "heard           : 2\n",
      "mad             : 2\n",
      "nervous         : 2\n",
      "things          : 2\n",
      "acute           : 1\n",
      "calmly          : 1\n",
      "destroyed       : 1\n",
      "disease         : 1\n",
      "dreadfully      : 1\n",
      "dulled          : 1\n",
      "earth           : 1\n",
      "healthily       : 1\n",
      "hearing         : 1\n",
      "hearken         : 1\n",
      "heaven          : 1\n",
      "hell            : 1\n",
      "observe         : 1\n",
      "say             : 1\n",
      "sense           : 1\n",
      "senses          : 1\n",
      "sharpened       : 1\n",
      "story           : 1\n",
      "tell            : 1\n",
      "true            : 1\n"
     ]
    }
   ],
   "source": [
    "# Convert BoW matrix to an array\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Pair each word with its corresponding count\n",
    "word_counts = dict(zip(words, bow_array[0]))\n",
    "\n",
    "# Print each word and how many times it appears in the text into a table-like representation\n",
    "print(\"ğŸ“Š Word Count (Bag-of-Words Representation):\")\n",
    "    \n",
    "# Sort words by frequency (descending)\n",
    "for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word:<15} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90405ab6-4cb3-4b7f-973a-0def4fe8e03f",
   "metadata": {},
   "source": [
    "# ğŸ§ª TEST 3 â€” Check if the word count dictionary works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a34ade5-2c38-4dd4-b55a-385408a7f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜„ PASSED: Word counts generated successfully!\n",
      "ğŸ† Top 5 most frequent words:\n",
      "heard          : 2\n",
      "mad            : 2\n",
      "nervous        : 2\n",
      "things         : 2\n",
      "acute          : 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if isinstance(word_counts, dict) and len(word_counts) > 5:\n",
    "        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"ğŸ˜„ PASSED: Word counts generated successfully!\")\n",
    "        print(\"ğŸ† Top 5 most frequent words:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"{word:<15}: {count}\")\n",
    "    else:\n",
    "        print(\"âŒ TRY AGAIN: Your word_counts dictionary seems empty or not created correctly.\")\n",
    "except Exception:\n",
    "    print(\"âŒ ERROR: Check if you defined 'word_counts' after creating the BoW matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93798f6b-7aee-4d68-ba6b-1ebeee179546",
   "metadata": {},
   "source": [
    "# ğŸ” What Does the Result Tell You about The Tell-Tale Heart?\n",
    "---\n",
    "\n",
    "> ğŸ§ **Reflect:** Does it give you a glimpse of what the short story is about? Would you consider it accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cce366-9aa6-4d25-b4ae-1dc53011a442",
   "metadata": {},
   "source": [
    "# ğŸ’­ Part 4: More Advanced NLP Models\n",
    "\n",
    "**Learning Outcomes (LO5 & LO6):**  \n",
    "\n",
    "Think critically about what you learned and its implications: Identify the core mechanics of NLP and its range of practical applications & Reflect on the technology learned and the learning experience to critically evaluate NLP. \n",
    "\n",
    "### ğŸ§© Try to find 1 or 2 alternative models beyond BoW for subject indexing\n",
    "For each, list at least **1 strength** and **1 limitation**\n",
    "\n",
    "**Examples to consider (pick any):**  \n",
    "- TF-IDF\n",
    "- BM25  \n",
    "- LSA / LDA (topic models)  \n",
    "- word2vec / doc2vec  \n",
    "- fastText  \n",
    "- Sentence-BERT (sentence embeddings)  \n",
    "- Transformer classifiers / zero-shot classification\n",
    "\n",
    "> ğŸ’¬ Post your findings on Padlet (https://padlet.com/zhaoruitong2002/LIBR547_NLP_Training_Session) and feel free to discuss with peers in class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
