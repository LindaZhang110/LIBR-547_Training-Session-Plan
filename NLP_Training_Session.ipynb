{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097d9fcb-16d5-4744-87e6-7653971788ee",
   "metadata": {},
   "source": [
    "# üåê Part 1: Environment Setup and Library Imports\n",
    "\n",
    "**Learning Outcome (LO1):**  \n",
    "You will learn to set up a Python environment, import essential libraries, and understand what each does.  \n",
    "\n",
    "> üí° *Tip:* We are using **UBC Syzygy** ‚Äî a cloud Jupyter Notebook environment. No installation is needed, but we must import libraries before coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad60a-c1e3-494c-bf61-30a66d40f562",
   "metadata": {},
   "source": [
    "# üì¶ Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547e28a4-0d84-469c-83eb-083c77f7f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (2.2.6)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.13/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy nltk scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7daf8c-a58f-41e8-8ab6-a4183cea2cb0",
   "metadata": {},
   "source": [
    "# üß∞ Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e936e781-708d-404e-a539-546953452de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jupyter/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully! üòÑ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import the Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download('punkt_tab') \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Import tools for tokenization and lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import the Counter class for word frequency counting\n",
    "from collections import Counter  \n",
    "\n",
    "# Import scikit-learn for Bag-of-Words (BoW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "# Download necessary NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully! üòÑ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8204b9d2-04be-41c4-bf3b-9c1f62bf0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4810e1-6d99-47e5-a6c5-bcfc90442bf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# üßπ Part 2: Text Cleaning and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b5ff3-539e-4e7f-8f33-542fac3307dc",
   "metadata": {},
   "source": [
    "# üìù Step 1: Load the sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfd755-a8a0-4c33-8e15-4265dfdc3bcb",
   "metadata": {},
   "source": [
    "### üìñ The Tell-Tale Heart by Edgar Allan Poe\n",
    "#### Source: https://poemuseum.org/the-tell-tale-heart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec242e5-7768-41c2-a8aa-b57a418ab507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text loaded successfully! Length: 398 characters\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "TRUE! ‚Äî nervous ‚Äî very, very dreadfully nervous I had been and am; but why will you say that I am mad?\n",
    "The disease had sharpened my senses ‚Äî not destroyed ‚Äî not dulled them. Above all was the sense of hearing acute.\n",
    "I heard all things in the heaven and in the earth. I heard many things in hell.\n",
    "How, then, am I mad? Hearken! and observe how healthily ‚Äî how calmly I can tell you the whole story.\n",
    "\"\"\"\n",
    "print(\"‚úÖ Text loaded successfully! Length:\", len(sample_text), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb7586-adff-4d3d-a04e-9b65bae389d9",
   "metadata": {},
   "source": [
    "# üß© Step 2: Tokenize text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d238aa6c-b2bc-423b-963e-dd8f6aa34e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRUE',\n",
       " '!',\n",
       " '‚Äî',\n",
       " 'nervous',\n",
       " '‚Äî',\n",
       " 'very',\n",
       " ',',\n",
       " 'very',\n",
       " 'dreadfully',\n",
       " 'nervous',\n",
       " 'I',\n",
       " 'had',\n",
       " 'been',\n",
       " 'and',\n",
       " 'am',\n",
       " ';',\n",
       " 'but',\n",
       " 'why',\n",
       " 'will',\n",
       " 'you',\n",
       " 'say',\n",
       " 'that',\n",
       " 'I',\n",
       " 'am',\n",
       " 'mad',\n",
       " '?',\n",
       " 'The',\n",
       " 'disease',\n",
       " 'had',\n",
       " 'sharpened',\n",
       " 'my',\n",
       " 'senses',\n",
       " '‚Äî',\n",
       " 'not',\n",
       " 'destroyed',\n",
       " '‚Äî',\n",
       " 'not',\n",
       " 'dulled',\n",
       " 'them',\n",
       " '.',\n",
       " 'Above',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'hearing',\n",
       " 'acute',\n",
       " '.',\n",
       " 'I',\n",
       " 'heard',\n",
       " 'all',\n",
       " 'things',\n",
       " 'in',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " 'I',\n",
       " 'heard',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'hell',\n",
       " '.',\n",
       " 'How',\n",
       " ',',\n",
       " 'then',\n",
       " ',',\n",
       " 'am',\n",
       " 'I',\n",
       " 'mad',\n",
       " '?',\n",
       " 'Hearken',\n",
       " '!',\n",
       " 'and',\n",
       " 'observe',\n",
       " 'how',\n",
       " 'healthily',\n",
       " '‚Äî',\n",
       " 'how',\n",
       " 'calmly',\n",
       " 'I',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HINT: What do we want to use word_tokenize() to tokenize?\n",
    "tokens = word_tokenize(____)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694b0c7e-4b2c-4cbc-94a5-7b0ea1b84e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TRUE', 'NN'),\n",
       " ('!', '.'),\n",
       " ('‚Äî', 'RB'),\n",
       " ('nervous', 'JJ'),\n",
       " ('‚Äî', 'JJ'),\n",
       " ('very', 'RB'),\n",
       " (',', ','),\n",
       " ('very', 'RB'),\n",
       " ('dreadfully', 'RB'),\n",
       " ('nervous', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('had', 'VBD'),\n",
       " ('been', 'VBN'),\n",
       " ('and', 'CC'),\n",
       " ('am', 'VBP'),\n",
       " (';', ':'),\n",
       " ('but', 'CC'),\n",
       " ('why', 'WRB'),\n",
       " ('will', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('say', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('mad', 'JJ'),\n",
       " ('?', '.'),\n",
       " ('The', 'DT'),\n",
       " ('disease', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('sharpened', 'VBN'),\n",
       " ('my', 'PRP$'),\n",
       " ('senses', 'NNS'),\n",
       " ('‚Äî', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('destroyed', 'VBN'),\n",
       " ('‚Äî', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('dulled', 'VB'),\n",
       " ('them', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Above', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('sense', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('hearing', 'VBG'),\n",
       " ('acute', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('heard', 'VBD'),\n",
       " ('all', 'DT'),\n",
       " ('things', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('heaven', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('earth', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('heard', 'VBD'),\n",
       " ('many', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('hell', 'NN'),\n",
       " ('.', '.'),\n",
       " ('How', 'NNP'),\n",
       " (',', ','),\n",
       " ('then', 'RB'),\n",
       " (',', ','),\n",
       " ('am', 'VBP'),\n",
       " ('I', 'PRP'),\n",
       " ('mad', 'JJ'),\n",
       " ('?', '.'),\n",
       " ('Hearken', 'NNP'),\n",
       " ('!', '.'),\n",
       " ('and', 'CC'),\n",
       " ('observe', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('healthily', 'RB'),\n",
       " ('‚Äî', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('calmly', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('tell', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('the', 'DT'),\n",
       " ('whole', 'JJ'),\n",
       " ('story', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HINT: you want to POS tag tokens\n",
    "tagged = pos_tag(____)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd905d-cf69-4969-ac98-ef13bae4f3b6",
   "metadata": {},
   "source": [
    "# üß© Step 3: Lemmatize each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8bc405-2edc-40a9-ba0d-9c9b0ca223be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: What do you want the lammatize to do here?\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer._____(word, get_wordnet_pos(tag)) for (word, tag) in tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea9860d-c678-4723-a858-e2b979793579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§Original Token      ‚Üí  üß† Lemma\n",
      "------------------------------\n",
      "TRUE            ‚Üí TRUE\n",
      "!               ‚Üí !\n",
      "‚Äî               ‚Üí ‚Äî\n",
      "nervous         ‚Üí nervous\n",
      "‚Äî               ‚Üí ‚Äî\n",
      "very            ‚Üí very\n",
      ",               ‚Üí ,\n",
      "very            ‚Üí very\n",
      "dreadfully      ‚Üí dreadfully\n",
      "nervous         ‚Üí nervous\n",
      "I               ‚Üí I\n",
      "had             ‚Üí have\n",
      "been            ‚Üí be\n",
      "and             ‚Üí and\n",
      "am              ‚Üí be\n",
      ";               ‚Üí ;\n",
      "but             ‚Üí but\n",
      "why             ‚Üí why\n",
      "will            ‚Üí will\n",
      "you             ‚Üí you\n",
      "say             ‚Üí say\n",
      "that            ‚Üí that\n",
      "I               ‚Üí I\n",
      "am              ‚Üí be\n",
      "mad             ‚Üí mad\n",
      "?               ‚Üí ?\n",
      "The             ‚Üí The\n",
      "disease         ‚Üí disease\n",
      "had             ‚Üí have\n",
      "sharpened       ‚Üí sharpen\n",
      "my              ‚Üí my\n",
      "senses          ‚Üí sens\n",
      "‚Äî               ‚Üí ‚Äî\n",
      "not             ‚Üí not\n",
      "destroyed       ‚Üí destroy\n",
      "‚Äî               ‚Üí ‚Äî\n",
      "not             ‚Üí not\n",
      "dulled          ‚Üí dull\n",
      "them            ‚Üí them\n",
      ".               ‚Üí .\n",
      "Above           ‚Üí Above\n",
      "all             ‚Üí all\n",
      "was             ‚Üí be\n",
      "the             ‚Üí the\n",
      "sense           ‚Üí sense\n",
      "of              ‚Üí of\n",
      "hearing         ‚Üí hear\n",
      "acute           ‚Üí acute\n",
      ".               ‚Üí .\n",
      "I               ‚Üí I\n",
      "heard           ‚Üí hear\n",
      "all             ‚Üí all\n",
      "things          ‚Üí thing\n",
      "in              ‚Üí in\n",
      "the             ‚Üí the\n",
      "heaven          ‚Üí heaven\n",
      "and             ‚Üí and\n",
      "in              ‚Üí in\n",
      "the             ‚Üí the\n",
      "earth           ‚Üí earth\n",
      ".               ‚Üí .\n",
      "I               ‚Üí I\n",
      "heard           ‚Üí hear\n",
      "many            ‚Üí many\n",
      "things          ‚Üí thing\n",
      "in              ‚Üí in\n",
      "hell            ‚Üí hell\n",
      ".               ‚Üí .\n",
      "How             ‚Üí How\n",
      ",               ‚Üí ,\n",
      "then            ‚Üí then\n",
      ",               ‚Üí ,\n",
      "am              ‚Üí be\n",
      "I               ‚Üí I\n",
      "mad             ‚Üí mad\n",
      "?               ‚Üí ?\n",
      "Hearken         ‚Üí Hearken\n",
      "!               ‚Üí !\n",
      "and             ‚Üí and\n",
      "observe         ‚Üí observe\n",
      "how             ‚Üí how\n",
      "healthily       ‚Üí healthily\n",
      "‚Äî               ‚Üí ‚Äî\n",
      "how             ‚Üí how\n",
      "calmly          ‚Üí calmly\n",
      "I               ‚Üí I\n",
      "can             ‚Üí can\n",
      "tell            ‚Üí tell\n",
      "you             ‚Üí you\n",
      "the             ‚Üí the\n",
      "whole           ‚Üí whole\n",
      "story           ‚Üí story\n",
      ".               ‚Üí .\n"
     ]
    }
   ],
   "source": [
    "# Print title/header\n",
    "print(\"üî§Original Token      ‚Üí  üß† Lemma\")\n",
    "print(\"-\" * 30)  # Optional: a separator line\n",
    "\n",
    "#HINT: Zip original tokens and their lemmas\n",
    "for original, lemma in ____(tokens, lemmas):\n",
    "    print(f\"{original:15} ‚Üí {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca8deb-6041-48ed-862e-6d55f1fc5bb1",
   "metadata": {},
   "source": [
    "# üîç Can You Spot the Difference?\n",
    "\n",
    "**Learning Outcome (LO2):**  \n",
    "Perform tokenization using lemmatization to clean up text for NLP.\n",
    "\n",
    "---\n",
    "\n",
    "> üßê **Think:** What has changed from Original Token ‚Üí Lemma?  \n",
    "> ü§î **Think:** How can this possibly help natural language processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347534d-c7af-44e4-a961-90ce88d9a863",
   "metadata": {},
   "source": [
    "# ‚úÖ TEST 1: Check if lemmatization was applied correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13268115-98e5-4470-b409-fbe00b58eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÑ PASSED: Your text was tokenized and lemmatized correctly!\n",
      "üî¢ Total tokens found: 93\n",
      "üß† Sample lemmas: ['TRUE', '!', '‚Äî', 'nervous', '‚Äî', 'very', ',', 'very', 'dreadfully', 'nervous']\n"
     ]
    }
   ],
   "source": [
    "# Basic checks\n",
    "conditions = [\n",
    "    isinstance(tokens, list),                    # tokens should be a list\n",
    "    len(tokens) > 20,                            # enough tokens should be found\n",
    "    isinstance(lemmas, list),                    # lemmas should be a list\n",
    "    all(isinstance(x, str) for x in lemmas),     # all lemmas should be strings\n",
    "]\n",
    "\n",
    "# Display friendly feedback\n",
    "if all(conditions):\n",
    "    print(\"üòÑ PASSED: Your text was tokenized and lemmatized correctly!\")\n",
    "    print(f\"üî¢ Total tokens found: {len(tokens)}\")\n",
    "    print(f\"üß† Sample lemmas: {lemmas[:10]}\")\n",
    "else:\n",
    "    print(\"‚ùå TRY AGAIN: Something's off. Check if you filled in the blanks correctly for tokenization or lemmatization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da667b-b2fc-4551-addc-7be224394146",
   "metadata": {},
   "source": [
    "# üß∞ Part 3: Bag-of-Words (BoW) Model\n",
    "\n",
    "**Learning Outcome (LO3):**  \n",
    "Understand how to represent text numerically using the Bag-of-Words model.\n",
    "\n",
    "> **Your task:** Fill in blanks to transform text into a matrix of word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c509830-d424-4cfb-8678-87a7cef83906",
   "metadata": {},
   "source": [
    "# üß© Step 1: Create a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c004ad3-f12e-49f8-9bf7-e50b69e7acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f815c9-1763-489f-b1c5-754a72f7607b",
   "metadata": {},
   "source": [
    "# üß© Step 2: Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0a70515-c075-4c02-a87d-c5bb6fd67e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: What is the text have you been focusing on so far?\n",
    "X = vectorizer.fit_transform([____])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400566c-2422-4155-aed4-3b80f717da8b",
   "metadata": {},
   "source": [
    "# üß© Step 3: Get feature (word) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2c2dc8-4210-4ad3-9001-a299e905882d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['above', 'acute', 'all', 'am', 'and', 'been', 'but', 'calmly',\n",
       "       'can', 'destroyed', 'disease', 'dreadfully', 'dulled', 'earth',\n",
       "       'had', 'healthily', 'heard', 'hearing', 'hearken', 'heaven',\n",
       "       'hell', 'how', 'in', 'mad', 'many', 'my', 'nervous', 'not',\n",
       "       'observe', 'of', 'say', 'sense', 'senses', 'sharpened', 'story',\n",
       "       'tell', 'that', 'the', 'them', 'then', 'things', 'true', 'very',\n",
       "       'was', 'whole', 'why', 'will', 'you'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names_out()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d4f01a-8b74-4ec5-b374-05dcd8bdfc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Vocabulary: ['above' 'acute' 'all' 'am' 'and' 'been' 'but' 'calmly' 'can' 'destroyed'\n",
      " 'disease' 'dreadfully' 'dulled' 'earth' 'had' 'healthily' 'heard'\n",
      " 'hearing' 'hearken' 'heaven' 'hell' 'how' 'in' 'mad' 'many' 'my'\n",
      " 'nervous' 'not' 'observe' 'of' 'say' 'sense' 'senses' 'sharpened' 'story'\n",
      " 'tell' 'that' 'the' 'them' 'then' 'things' 'true' 'very' 'was' 'whole'\n",
      " 'why' 'will' 'you']\n",
      "üßÆ BoW Matrix:\n",
      " [[1 1 2 3 3 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 3 3 2 1 1 2 2 1 1 1 1 1 1 1 1\n",
      "  1 5 1 1 2 1 2 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"üî§ Vocabulary:\", words)\n",
    "print(\"üßÆ BoW Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69485f1-8bb2-4e16-a335-2c5bb0b74cf8",
   "metadata": {},
   "source": [
    "# üß™ TEST 2 ‚Äî Check if Bag-of-Words model is correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff01fa4-def0-4cab-a0e2-17017aa78e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÑ PASSED: Bag-of-Words created successfully!\n",
      "üìä Vocabulary size: 48\n",
      "üßÆ BoW matrix shape: (1, 48)\n",
      "üî§ Sample words: ['above' 'acute' 'all' 'am' 'and' 'been' 'but' 'calmly' 'can' 'destroyed']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_words = len(words)\n",
    "    matrix_shape = X.shape\n",
    "\n",
    "    if num_words > 10 and matrix_shape[1] == num_words:\n",
    "        print(\"üòÑ PASSED: Bag-of-Words created successfully!\")\n",
    "        print(f\"üìä Vocabulary size: {num_words}\")\n",
    "        print(f\"üßÆ BoW matrix shape: {matrix_shape}\")\n",
    "        print(f\"üî§ Sample words: {words[:10]}\")\n",
    "    else:\n",
    "        print(\"‚ùå TRY AGAIN: BoW model doesn't look right. Make sure to use vectorizer.fit_transform([sample_text]) and get_feature_names_out().\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå ERROR: Something went wrong.\")\n",
    "    print(\"üí° HINT: Check if 'vectorizer', 'X', and 'words' are defined correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69636f25-5631-4893-87d5-b30c84dd53aa",
   "metadata": {},
   "source": [
    "# üßæ Step 4: Display word counts so you can SEE how BoW works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cc0b4d6-2ae8-47ce-8b90-eb9793d8dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Word Count (Bag-of-Words Representation):\n",
      "the             : 5\n",
      "am              : 3\n",
      "and             : 3\n",
      "how             : 3\n",
      "in              : 3\n",
      "all             : 2\n",
      "had             : 2\n",
      "heard           : 2\n",
      "mad             : 2\n",
      "nervous         : 2\n",
      "not             : 2\n",
      "things          : 2\n",
      "very            : 2\n",
      "you             : 2\n",
      "above           : 1\n",
      "acute           : 1\n",
      "been            : 1\n",
      "but             : 1\n",
      "calmly          : 1\n",
      "can             : 1\n",
      "destroyed       : 1\n",
      "disease         : 1\n",
      "dreadfully      : 1\n",
      "dulled          : 1\n",
      "earth           : 1\n",
      "healthily       : 1\n",
      "hearing         : 1\n",
      "hearken         : 1\n",
      "heaven          : 1\n",
      "hell            : 1\n",
      "many            : 1\n",
      "my              : 1\n",
      "observe         : 1\n",
      "of              : 1\n",
      "say             : 1\n",
      "sense           : 1\n",
      "senses          : 1\n",
      "sharpened       : 1\n",
      "story           : 1\n",
      "tell            : 1\n",
      "that            : 1\n",
      "them            : 1\n",
      "then            : 1\n",
      "true            : 1\n",
      "was             : 1\n",
      "whole           : 1\n",
      "why             : 1\n",
      "will            : 1\n"
     ]
    }
   ],
   "source": [
    "# Convert BoW matrix to an array\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Pair each word with its corresponding count\n",
    "word_counts = dict(zip(words, bow_array[0]))\n",
    "\n",
    "# Print each word and how many times it appears in the text\n",
    "print(\"üìä Word Count (Bag-of-Words Representation):\")\n",
    "    \n",
    "# Sort words by frequency (descending)\n",
    "for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word:<15} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a34ade5-2c38-4dd4-b55a-385408a7f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÑ PASSED: Word counts generated successfully!\n",
      "üèÜ Top 5 most frequent words:\n",
      "the            : 5\n",
      "am             : 3\n",
      "and            : 3\n",
      "how            : 3\n",
      "in             : 3\n"
     ]
    }
   ],
   "source": [
    "# üß™ TEST 3 ‚Äî Check if word count dictionary works\n",
    "\n",
    "try:\n",
    "    if isinstance(word_counts, dict) and len(word_counts) > 5:\n",
    "        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"üòÑ PASSED: Word counts generated successfully!\")\n",
    "        print(\"üèÜ Top 5 most frequent words:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"{word:<15}: {count}\")\n",
    "    else:\n",
    "        print(\"‚ùå TRY AGAIN: Your word_counts dictionary seems empty or not created correctly.\")\n",
    "except Exception:\n",
    "    print(\"‚ùå ERROR: Check if you defined 'word_counts' after creating the BoW matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93798f6b-7aee-4d68-ba6b-1ebeee179546",
   "metadata": {},
   "source": [
    "# üí≠ Part 4: Reflection & Real-World Applications\n",
    "\n",
    "**Learning Outcome (LO5 & LO6):**  \n",
    "Think critically about what you learned and its implications.\n",
    "\n",
    "### üß© Discussion Prompts\n",
    "1. Why is Bag-of-Words useful yet limited for real-world NLP tasks?\n",
    "2. Can you name an industry where NLP can be transformative?\n",
    "3. Reflect on your learning process ‚Äî how did collaboration help?\n",
    "\n",
    "> üí¨ Post your reflections on Padlet or discuss in class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
