{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097d9fcb-16d5-4744-87e6-7653971788ee",
   "metadata": {},
   "source": [
    "# 🌐 Part 1: Environment Setup and Library Imports\n",
    "\n",
    "**Learning Outcome (LO1):**  \n",
    "You will learn to set up a Python environment, import essential libraries, and understand what each does.  \n",
    "\n",
    "> 💡 *Tip:* We are using **UBC Syzygy** — a cloud Jupyter Notebook environment. No installation is needed, but we must import libraries before coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad60a-c1e3-494c-bf61-30a66d40f562",
   "metadata": {},
   "source": [
    "# 📦 Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547e28a4-0d84-469c-83eb-083c77f7f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (2.2.6)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.13/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy nltk scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7daf8c-a58f-41e8-8ab6-a4183cea2cb0",
   "metadata": {},
   "source": [
    "# 🧰 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e936e781-708d-404e-a539-546953452de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully! 😄\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import the Natural Language Toolkit\n",
    "import nltk  \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Import tools for tokenization and lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import the Counter class for word frequency counting\n",
    "from collections import Counter  \n",
    "\n",
    "# Import scikit-learn for Bag-of-Words (BoW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "# Download necessary NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"✅ Libraries imported successfully! 😄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8204b9d2-04be-41c4-bf3b-9c1f62bf0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4810e1-6d99-47e5-a6c5-bcfc90442bf7",
   "metadata": {},
   "source": [
    "# 🧹 Part 2: Text Cleaning and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b5ff3-539e-4e7f-8f33-542fac3307dc",
   "metadata": {},
   "source": [
    "# 📝 Step 1: Load the sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfd755-a8a0-4c33-8e15-4265dfdc3bcb",
   "metadata": {},
   "source": [
    "### 📖 The Tell-Tale Heart by Edgar Allan Poe\n",
    "#### Source: https://poemuseum.org/the-tell-tale-heart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec242e5-7768-41c2-a8aa-b57a418ab507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text loaded successfully! Length: 398 characters\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "TRUE! — nervous — very, very dreadfully nervous I had been and am; but why will you say that I am mad?\n",
    "The disease had sharpened my senses — not destroyed — not dulled them. Above all was the sense of hearing acute.\n",
    "I heard all things in the heaven and in the earth. I heard many things in hell.\n",
    "How, then, am I mad? Hearken! and observe how healthily — how calmly I can tell you the whole story.\n",
    "\"\"\"\n",
    "print(\"✅ Text loaded successfully! Length:\", len(sample_text), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb7586-adff-4d3d-a04e-9b65bae389d9",
   "metadata": {},
   "source": [
    "# 🧩 Step 2: Tokenize text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d238aa6c-b2bc-423b-963e-dd8f6aa34e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: use word_tokenize()\n",
    "tokens = ___\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "694b0c7e-4b2c-4cbc-94a5-7b0ea1b84e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: you want to POS tag tokens\n",
    "tagged = pos_tag(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd905d-cf69-4969-ac98-ef13bae4f3b6",
   "metadata": {},
   "source": [
    "# 🧩 Step 3: Lemmatize each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec8bc405-2edc-40a9-ba0d-9c9b0ca223be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetLemmatizer' object has no attribute '___'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# HINT: what so you want the lammatize to do here?\u001b[39;00m\n\u001b[32m      2\u001b[39m lemmatizer = WordNetLemmatizer()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m lemmas = [\u001b[43mlemmatizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m___\u001b[49m(word, get_wordnet_pos(tag)) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[31mAttributeError\u001b[39m: 'WordNetLemmatizer' object has no attribute '___'"
     ]
    }
   ],
   "source": [
    "# HINT: what so you want the lammatize to do here?\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.___(word, get_wordnet_pos(tag)) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cea9860d-c678-4723-a858-e2b979793579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤Original Token      →  🧠 Lemma\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)  \u001b[38;5;66;03m# Optional: a separator line\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#HINT: Zip original tokens and their lemmas\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m original, lemma \u001b[38;5;129;01min\u001b[39;00m \u001b[43m___\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmas\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemma\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "# Print title/header\n",
    "print(\"🔤Original Token      →  🧠 Lemma\")\n",
    "print(\"-\" * 30)  # Optional: a separator line\n",
    "\n",
    "#HINT: Zip original tokens and their lemmas\n",
    "for original, lemma in ___(tokens, lemmas):\n",
    "    print(f\"{original:15} → {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca8deb-6041-48ed-862e-6d55f1fc5bb1",
   "metadata": {},
   "source": [
    "# 🔍 Can You Spot the Difference?\n",
    "\n",
    "**Learning Outcome (LO2):**  \n",
    "Perform tokenization using lemmatization to clean up text for NLP.\n",
    "\n",
    "---\n",
    "\n",
    "> 🧐 **Think:** What has changed from Original Token → Lemma  \n",
    "> 🤔 **Think:** How can this possibly help natural language processing?\n",
    "\n",
    "---\n",
    "\n",
    "NLP is a subfield of artificial intelligence, specifically associated with machine learning, and plays an important role in enabling computers to understand, interpret, and communicate with human natural language (Kumar, 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347534d-c7af-44e4-a961-90ce88d9a863",
   "metadata": {},
   "source": [
    "# ✅ TEST 1: Check if lemmatization was applied correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13268115-98e5-4470-b409-fbe00b58eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄 PASSED: Your text was tokenized and lemmatized correctly!\n",
      "🔢 Total tokens found: 93\n",
      "🧠 Sample lemmas: ['TRUE', '!', '—', 'nervous', '—', 'very', ',', 'very', 'dreadfully', 'nervous']\n"
     ]
    }
   ],
   "source": [
    "# Basic checks\n",
    "conditions = [\n",
    "    isinstance(tokens, list),                    # tokens should be a list\n",
    "    len(tokens) > 20,                            # enough tokens should be found\n",
    "    isinstance(lemmas, list),                    # lemmas should be a list\n",
    "    all(isinstance(x, str) for x in lemmas),     # all lemmas should be strings\n",
    "]\n",
    "\n",
    "# Display friendly feedback\n",
    "if all(conditions):\n",
    "    print(\"😄 PASSED: Your text was tokenized and lemmatized correctly!\")\n",
    "    print(f\"🔢 Total tokens found: {len(tokens)}\")\n",
    "    print(f\"🧠 Sample lemmas: {lemmas[:10]}\")\n",
    "else:\n",
    "    print(\"❌ TRY AGAIN: Something's off. Check if you filled in the blanks correctly for tokenization or lemmatization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da667b-b2fc-4551-addc-7be224394146",
   "metadata": {},
   "source": [
    "# 🧰 Part 3: Bag-of-Words (BoW) Model\n",
    "\n",
    "**Learning Outcome (LO3):**  \n",
    "Understand how to represent text numerically using the Bag-of-Words model.\n",
    "\n",
    "> 🧩 **Your task:** Fill in blanks to transform text into a matrix of word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c509830-d424-4cfb-8678-87a7cef83906",
   "metadata": {},
   "source": [
    "# 🧩 Step 1: Create a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c004ad3-f12e-49f8-9bf7-e50b69e7acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f815c9-1763-489f-b1c5-754a72f7607b",
   "metadata": {},
   "source": [
    "# 🧩 Step 2: Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a70515-c075-4c02-a87d-c5bb6fd67e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: vectorizer.fit_transform([])\n",
    "X = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400566c-2422-4155-aed4-3b80f717da8b",
   "metadata": {},
   "source": [
    "# 🧩 Step 3: Get feature (word) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2c2dc8-4210-4ad3-9001-a299e905882d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['above', 'acute', 'all', 'am', 'and', 'been', 'but', 'calmly',\n",
       "       'can', 'destroyed', 'disease', 'dreadfully', 'dulled', 'earth',\n",
       "       'had', 'healthily', 'heard', 'hearing', 'hearken', 'heaven',\n",
       "       'hell', 'how', 'in', 'mad', 'many', 'my', 'nervous', 'not',\n",
       "       'observe', 'of', 'say', 'sense', 'senses', 'sharpened', 'story',\n",
       "       'tell', 'that', 'the', 'them', 'then', 'things', 'true', 'very',\n",
       "       'was', 'whole', 'why', 'will', 'you'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names_out()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d4f01a-8b74-4ec5-b374-05dcd8bdfc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Vocabulary: ['above' 'acute' 'all' 'am' 'and' 'been' 'but' 'calmly' 'can' 'destroyed'\n",
      " 'disease' 'dreadfully' 'dulled' 'earth' 'had' 'healthily' 'heard'\n",
      " 'hearing' 'hearken' 'heaven' 'hell' 'how' 'in' 'mad' 'many' 'my'\n",
      " 'nervous' 'not' 'observe' 'of' 'say' 'sense' 'senses' 'sharpened' 'story'\n",
      " 'tell' 'that' 'the' 'them' 'then' 'things' 'true' 'very' 'was' 'whole'\n",
      " 'why' 'will' 'you']\n",
      "🧮 BoW Matrix:\n",
      " [[1 1 2 3 3 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 3 3 2 1 1 2 2 1 1 1 1 1 1 1 1\n",
      "  1 5 1 1 2 1 2 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"🔤 Vocabulary:\", words)\n",
    "print(\"🧮 BoW Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69485f1-8bb2-4e16-a335-2c5bb0b74cf8",
   "metadata": {},
   "source": [
    "# 🧪 TEST 2 — Check if Bag-of-Words model is correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff01fa4-def0-4cab-a0e2-17017aa78e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄 PASSED: Bag-of-Words created successfully!\n",
      "📊 Vocabulary size: 48\n",
      "🧮 BoW matrix shape: (1, 48)\n",
      "🔤 Sample words: ['above' 'acute' 'all' 'am' 'and' 'been' 'but' 'calmly' 'can' 'destroyed']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_words = len(words)\n",
    "    matrix_shape = X.shape\n",
    "\n",
    "    if num_words > 10 and matrix_shape[1] == num_words:\n",
    "        print(\"😄 PASSED: Bag-of-Words created successfully!\")\n",
    "        print(f\"📊 Vocabulary size: {num_words}\")\n",
    "        print(f\"🧮 BoW matrix shape: {matrix_shape}\")\n",
    "        print(f\"🔤 Sample words: {words[:10]}\")\n",
    "    else:\n",
    "        print(\"❌ TRY AGAIN: BoW model doesn't look right. Make sure to use vectorizer.fit_transform([sample_text]) and get_feature_names_out().\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ ERROR: Something went wrong.\")\n",
    "    print(\"💡 HINT: Check if 'vectorizer', 'X', and 'words' are defined correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69636f25-5631-4893-87d5-b30c84dd53aa",
   "metadata": {},
   "source": [
    "# 🧾 Step 4: Display word counts so you can SEE how BoW works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc0b4d6-2ae8-47ce-8b90-eb9793d8dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Word Count (Bag-of-Words Representation):\n",
      "the             : 5\n",
      "am              : 3\n",
      "and             : 3\n",
      "how             : 3\n",
      "in              : 3\n",
      "all             : 2\n",
      "had             : 2\n",
      "heard           : 2\n",
      "mad             : 2\n",
      "nervous         : 2\n",
      "not             : 2\n",
      "things          : 2\n",
      "very            : 2\n",
      "you             : 2\n",
      "above           : 1\n",
      "acute           : 1\n",
      "been            : 1\n",
      "but             : 1\n",
      "calmly          : 1\n",
      "can             : 1\n",
      "destroyed       : 1\n",
      "disease         : 1\n",
      "dreadfully      : 1\n",
      "dulled          : 1\n",
      "earth           : 1\n",
      "healthily       : 1\n",
      "hearing         : 1\n",
      "hearken         : 1\n",
      "heaven          : 1\n",
      "hell            : 1\n",
      "many            : 1\n",
      "my              : 1\n",
      "observe         : 1\n",
      "of              : 1\n",
      "say             : 1\n",
      "sense           : 1\n",
      "senses          : 1\n",
      "sharpened       : 1\n",
      "story           : 1\n",
      "tell            : 1\n",
      "that            : 1\n",
      "them            : 1\n",
      "then            : 1\n",
      "true            : 1\n",
      "was             : 1\n",
      "whole           : 1\n",
      "why             : 1\n",
      "will            : 1\n"
     ]
    }
   ],
   "source": [
    "# Convert BoW matrix to an array\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Pair each word with its corresponding count\n",
    "word_counts = dict(zip(words, bow_array[0]))\n",
    "\n",
    "# Print each word and how many times it appears in the text\n",
    "print(\"📊 Word Count (Bag-of-Words Representation):\")\n",
    "    \n",
    "# Sort words by frequency (descending)\n",
    "for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word:<15} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a34ade5-2c38-4dd4-b55a-385408a7f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄 PASSED: Word counts generated successfully!\n",
      "🏆 Top 5 most frequent words:\n",
      "the            : 5\n",
      "am             : 3\n",
      "and            : 3\n",
      "how            : 3\n",
      "in             : 3\n"
     ]
    }
   ],
   "source": [
    "# 🧪 TEST 3 — Check if word count dictionary works\n",
    "\n",
    "try:\n",
    "    if isinstance(word_counts, dict) and len(word_counts) > 5:\n",
    "        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"😄 PASSED: Word counts generated successfully!\")\n",
    "        print(\"🏆 Top 5 most frequent words:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"{word:<15}: {count}\")\n",
    "    else:\n",
    "        print(\"❌ TRY AGAIN: Your word_counts dictionary seems empty or not created correctly.\")\n",
    "except Exception:\n",
    "    print(\"❌ ERROR: Check if you defined 'word_counts' after creating the BoW matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93798f6b-7aee-4d68-ba6b-1ebeee179546",
   "metadata": {},
   "source": [
    "# 💭 Part 4: Reflection & Real-World Applications\n",
    "\n",
    "**Learning Outcome (LO5 & LO6):**  \n",
    "Think critically about what you learned and its implications.\n",
    "\n",
    "### 🧩 Discussion Prompts\n",
    "1. Why is Bag-of-Words useful yet limited for real-world NLP tasks?\n",
    "2. Can you name an industry where NLP can be transformative?\n",
    "3. Reflect on your learning process — how did collaboration help?\n",
    "\n",
    "> 💬 Post your reflections on **Padlet** or discuss in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196f3de-7110-49e4-b565-fb138aef4367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
